{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEaMX_-oEKMc"
      },
      "source": [
        "#Email subject generation - Instruct Version of Gemma-2b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Auto supports RoPE Scaling internally, via kaiokendev's method.\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4.\n",
        "load_in_4bit = True # Using 4bit quantization to reduce memory usage.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "                  \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "                  \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "                  \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "                  \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "                  \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "                  \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "                  \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "                  \"unsloth/mistral-7b-bnb-4bit\",\n",
        "                  \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "              ] # More models at https://huggingface.co/unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwtuQmvJQ7YY",
        "outputId": "00a933a9-1325-4265-c29e-2fdae0608322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.8: Fast Gemma patching. Transformers = 4.44.0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "              model_name = \"unsloth/gemma-2b-bnb-4bit\",\n",
        "              max_seq_length = max_seq_length,\n",
        "              dtype = dtype,\n",
        "              load_in_4bit = load_in_4bit,\n",
        "              # token = \"hf_...\", # For gated models (when using tokens to access the org specific models)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kzfbR52cG7rv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "##**LoRA adapters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "##LoRA adapters (Updates 1 to 10% of all parameters)\n",
        "## #\"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "                        model,\n",
        "                        r = 16, # (or 8, 16, 32, 64, 128)\n",
        "                        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "                        lora_alpha = 16,\n",
        "                        lora_dropout = 0, # 0 is optimized\n",
        "                        bias = \"none\",    # \"none\" is optimized\n",
        "                        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "                        random_state = 5000, # 3407,\n",
        "                        use_rslora = False,  # Using Rank stabilized LoRA\n",
        "                        loftq_config = None, # And LoftQ\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycyK5ndO1kwq",
        "outputId": "69f3d8a8-1d6d-4156-8d86-9d108aa98964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AESLC' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ryanzhumich/AESLC.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CRhJgb71ldf",
        "outputId": "3e366866-1651-42d4-ea55-96f640279dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file saved to /content/dataset.json\n"
          ]
        }
      ],
      "source": [
        "#Generate JSON file from email dataset\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define the folder containing the text files\n",
        "folder_path = '/content/AESLC/enron_subject_line/train'\n",
        "\n",
        "# Initialize lists to store the data\n",
        "data = []\n",
        "instruction = 'Please help summarize the provided email body and generate email subject'\n",
        "# Iterate over each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".subject\"):\n",
        "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            # Split the content into body and subject\n",
        "            if '@subject' in content:\n",
        "                body_text, subject_text = content.split('@subject')\n",
        "                data.append({\n",
        "                    'instruction': instruction,\n",
        "                    'input': body_text.strip(),\n",
        "                    'output': subject_text.strip()\n",
        "                })\n",
        "\n",
        "# Save the data to a JSON file\n",
        "json_path = '/content/dataset.json'\n",
        "with open(json_path, 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"JSON file saved to {json_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "##Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7ee519403c824836be36d233e17eab6d",
            "5ee688a29d294cf99bfa17b54142dec3",
            "77d6a1fb9dcf4bc48a3b801c7d916ddf",
            "fe7b4553187540a89fa384059919bafc",
            "7b2ec3f21f054834b84d34c8b5bd4e03",
            "9383f2dc2957476d8a72a4b001f51b71",
            "8a701b7cf899416b99c1d85495bac1ba",
            "51bac98b3825477ca835b9a1767e07a7",
            "e629b2d2c9da485a8575b9ff534ae069",
            "527743b06ac842c18f02f70083640735",
            "19cd3422ce9044a0891ba94b16de6932"
          ]
        },
        "collapsed": true,
        "id": "LjY75GoYUCB8",
        "outputId": "ab752d9a-62db-4ebc-fc91-6dc8f56d6a6c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14436 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ee519403c824836be36d233e17eab6d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "email_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # EOS_TOKEN, to limit generation to avoid forever generation!\n",
        "        text = email_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import Dataset\n",
        "import json\n",
        "# Load your custom dataset\n",
        "json_path = '/content/dataset.json'\n",
        "\n",
        "with open(json_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "#from datasets import load_dataset\n",
        "#dataset = load_dataset(\"/content/dataset.json\", split = \"train\")\n",
        "dataset = Dataset.from_list(data)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "##Train the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "4f66021b131f4f44b5cf72275285f021",
            "e78248c57ab74ad4885e9d3717afb463",
            "b8a43fe517a7491081c80946ab101787",
            "6971887d4bd74d228f7af9ae02959845",
            "f74040096be342bf8409732a3b4a4325",
            "db89bf602f6144b49a6451f18b5e6ce3",
            "51fd859e1bc44af2bf37a30702b6c37e",
            "7fddd59b738742998abc8627be17683e",
            "13cd6061f14d44b7a3981dd615a98d72",
            "866f6e10849d4b8ab9634cc5cba28315",
            "5d4a573ee3714f46a7e4260a2ad78b42"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "af42f944-6f25-45af-e144-c2e020c59dbb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/14436 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f66021b131f4f44b5cf72275285f021"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "#Using Huggingface TRL's `SFTTrainer`[TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer).\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "                  per_device_train_batch_size = 2,\n",
        "                  gradient_accumulation_steps = 4,\n",
        "                  warmup_steps = 5,\n",
        "                  max_steps = 40, ##Tweaked from 60\n",
        "                  learning_rate = 2e-4,\n",
        "                  fp16 = not is_bfloat16_supported(),\n",
        "                  bf16 = is_bfloat16_supported(),\n",
        "                  logging_steps = 1,\n",
        "                  optim = \"adamw_8bit\",\n",
        "                  weight_decay = 0.01,\n",
        "                  lr_scheduler_type = \"linear\",\n",
        "                  seed = 5000, # 3407,\n",
        "                  output_dir = \"outputs\",\n",
        "              ),\n",
        ")\n",
        "\n",
        "#Try & set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. Support TRL's `DPOTrainer`!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXIR1Bzy2xUV"
      },
      "source": [
        "###Current memory stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "1fb5a5fc-f7d8-482f-fbc7-02d7f7009884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Memory Stats:\n",
            "GPU = Tesla T4\n",
            "Max memory = 14.748 GB\n",
            "Memory reserved = 4.506 GB\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"Current Memory Stats:\")\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "# print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "# print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "print(f\"GPU = {gpu_stats.name}\")\n",
        "print(f\"Max memory = {max_memory} GB\")\n",
        "print(f\"Memory reserved = {start_gpu_memory} GB\")\n",
        "print(\"-----------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "yqxqAZ7KJ4oL",
        "outputId": "97c8d177-76e7-493c-f37f-af6fe7abe07a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 14,436 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 40\n",
            " \"-____-\"     Number of trainable parameters = 19,611,648\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 01:52, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.259000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.407600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.212300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.275600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.980100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.591500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.110100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.928400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.855300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.643100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.212400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.439800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.537500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.138800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.016900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.139200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.227400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.255700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.246300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.958100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.147800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.124000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.046100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.837100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.134100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.980300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.183200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.087600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.080800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.907400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.869100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.204900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.138600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.956000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.030600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.065000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.976100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.909100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>2.253900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.051300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "ba836524-eff3-4db3-97b8-15f1c0a22ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Memory Stats:\n",
            "2.03 minutes used for training.\n",
            "121.5326 seconds used for training.\n",
            "Peak reserved memory = 8.48 GB.\n",
            "Peak reserved memory for training = 6.179 GB.\n",
            "Peak reserved memory % of max memory = 57.499 %.\n",
            "Peak reserved memory for training % of max memory = 41.897 %.\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(\"Training Memory Stats:\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "68dd36ee-9e44-4154-d54d-1c3dc023b272"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease help summarize the provided email body and generate email subject\\n\\n### Input:\\nKevin Presto is requesting that you attend a meeting regarding Organizing an Action Plan for the Start-up of Netco.\\nThe meeting will be held in ECS 06716 at 9:30 am, Wednesday, January 2, 2002.\\nFor Tim and Chris, could you please call 713-584-2067.\\nThis is the telephone number in the conference room.\\nIf you should have any questions, please call T Jae Black at 3-5800.\\nThanks\\n\\n### Response:\\nKevin Presto is requesting that you attend a meeting regarding Organizing an Action Plan for the Start-up of Netco.<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Run the model! Change the instruction & input, and leave output blank!\n",
        "#email_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) #Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    email_prompt.format(\n",
        "        \"Please help summarize the provided email body and generate email subject\", #Instruction\n",
        "        \"Kevin Presto is requesting that you attend a meeting regarding Organizing an Action Plan for the Start-up of Netco.\\nThe meeting will be held in ECS 06716 at 9:30 am, Wednesday, January 2, 2002.\\nFor Tim and Chris, could you please call 713-584-2067.\\nThis is the telephone number in the conference room.\\nIf you should have any questions, please call T Jae Black at 3-5800.\\nThanks\", #input\n",
        "        \"\", #output - leave this blank for generation!\n",
        "    )\n",
        "  ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)\n",
        "\n",
        "##Substring to fetch only response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "10592cfa-e225-4909-90f0-6eb033945fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Please help summarize the provided email body and generate email subject\n",
            "\n",
            "### Input:\n",
            "Kevin Presto is requesting that you attend a meeting regarding Organizing an Action Plan for the Start-up of Netco.\n",
            "The meeting will be held in ECS 06716 at 9:30 am, Wednesday, January 2, 2002.\n",
            "For Tim and Chris, could you please call 713-584-2067.\n",
            "This is the telephone number in the conference room.\n",
            "If you should have any questions, please call T Jae Black at 3-5800.\n",
            "Thanks\n",
            "\n",
            "### Response:\n",
            "Kevin Presto is requesting that you attend a meeting regarding Organizing an Action Plan for the Start-up of Netco.<eos>\n"
          ]
        }
      ],
      "source": [
        "#Use `TextStreamer` for continuous inference - to view generation token by token, instead of waiting for the entire duration.\n",
        "#email_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    email_prompt.format(\n",
        "        \"Please help summarize the provided email body and generate email subject\", # instruction\n",
        "        \"Kevin Presto is requesting that you attend a meeting regarding Organizing an Action Plan for the Start-up of Netco.\\nThe meeting will be held in ECS 06716 at 9:30 am, Wednesday, January 2, 2002.\\nFor Tim and Chris, could you please call 713-584-2067.\\nThis is the telephone number in the conference room.\\nIf you should have any questions, please call T Jae Black at 3-5800.\\nThanks\", #input\n",
        "\n",
        "        #\"Phillip,   Could you please do me a favor?I would like  to read your current title policy to see what it says about easements.You  should have received a copy during your closing.I don't know how many  pages it will be but let me know how you want to handle getting a copy  made.I'll be happy to make the copy, or whatever makes it easy for  you.Thanks,\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, use `save_pretrained` to save locally (otherwise Huggingface's `push_to_hub` to save online).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "58a09d52-f0b0-488f-9f48-41187f187916"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('EmailSubGen_Gemma2b_lora_model/tokenizer_config.json',\n",
              " 'EmailSubGen_Gemma2b_lora_model/special_tokens_map.json',\n",
              " 'EmailSubGen_Gemma2b_lora_model/tokenizer.model',\n",
              " 'EmailSubGen_Gemma2b_lora_model/added_tokens.json',\n",
              " 'EmailSubGen_Gemma2b_lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.save_pretrained(\"EmailSubGen_Gemma2b_lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"EmailSubGen_Gemma2b_lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdRp6xun5rxf",
        "outputId": "711fbabb-e63d-429a-af3e-f68cd333c87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.8: Fast Gemma patching. Transformers = 4.44.0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease help summarize the provided email body and generate email subject\\n\\n### Input:\\nThe following reports have been waiting for your approval for more than 4 days.Please review.Owner: James W Reitmeyer Report Name: JReitmeyer 10/24/01 Days In Mgr.Queue: 5\\n\\n### Response:\\nJReitmeyer 10/24/01 Days In Mgr.Queue: 5<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"EmailSubGen_Gemma2b_lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "        #load_in_8bit_fp32_cpu_offload=True, # Add this line to enable CPU offloading\n",
        "        device_map={\"\":0} # Add this line to specify GPU 0 for model placement\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    email_prompt.format(\n",
        "        \"Please help summarize the provided email body and generate email subject\", # instruction\n",
        "        \"The following reports have been waiting for your approval for more than 4 days.Please review.Owner: James W Reitmeyer Report Name: JReitmeyer 10/24/01 Days In Mgr.Queue: 5\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    ),\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWh4RFV696Yn"
      },
      "source": [
        "##Save model - locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6O0ll3f96vS",
        "outputId": "6469de9c-1178-4c81-b571-1d303d04b967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: /content/EmailSubGen_Gemma2_lora_model\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/EmailSubGen_Gemma2_lora_model.zip . -i /content/EmailSubGen_Gemma2_lora_model)\n",
            "Mounted at /content/drive\n",
            "cp: cannot stat '/content/EmailSubGen_Gemma2_lora_model.zip': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# prompt: zip folder /content/EmailSubGen_Gemma2_lora_model and upload to google drive\n",
        "\n",
        "!zip -r /content/EmailSubGen_Gemma2_lora_model.zip /content/EmailSubGen_Gemma2_lora_model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/EmailSubGen_Gemma2_lora_model.zip /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mf18H5L-Qfe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuyWCnmR-Qui",
        "outputId": "398aef14-4828-4b09-8211-8c8d6d9582ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file saved to /content/AESLC/enron_subject_line/test/testdataset.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define the folder containing the text files\n",
        "folder_path = '/content/AESLC/enron_subject_line/test'\n",
        "\n",
        "# Initialize lists to store the data\n",
        "data = []\n",
        "instruction = 'Please help summarize the provided email body and generate email subject'\n",
        "# Iterate over each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".subject\"):\n",
        "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            # Split the content into body and subject\n",
        "            if '@subject' in content:\n",
        "                body_text, subject_text = content.split('@subject')\n",
        "\n",
        "                lines = subject_text.strip().splitlines()  # Split by lines and remove leading/trailing whitespace\n",
        "                output = []\n",
        "                for line in lines:\n",
        "                    if line.strip():\n",
        "                        if line.startswith(\"@\"):\n",
        "                            annotation = line.split()[1:]  # Extract annotation text after \"@\" and split by space\n",
        "                            if len(annotation):\n",
        "                                output.append(\"\".join(annotation))  # Join words in annotation back together\n",
        "                        else:\n",
        "                            output.append(line.strip())  # Add subject or remaining text after removing whitespace\n",
        "                data.append({\n",
        "                    'instruction': instruction,\n",
        "                    'input': body_text.strip(),\n",
        "                    'output': output\n",
        "                })\n",
        "\n",
        "# Save the data to a JSON file\n",
        "json_path = '/content/AESLC/enron_subject_line/test/testdataset.json'\n",
        "with open(json_path, 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"JSON file saved to {json_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a5b61d4768d34ed48835e450de6d559e",
            "b60f8b553948480e812f22630de26751",
            "1236b75266ff49ffa0d86394f1abb999",
            "3acb04584353414b9eeeac2c0d0b4e42",
            "4d6ac5e2a1304bbb9947301045531b06",
            "9e777897fc5840a0aeae26eae70d868a",
            "ed580b749f7140dbbb1f2dc2d0e970c6",
            "07469b6eb8194f68824ef2d4aa2027cd",
            "da1525c981504bdb8da5338fa716591b",
            "5bfdf8fa01c94d129ded70f8d5f847a2",
            "2e0be2e3baaa424ea13913b41902b2fa"
          ]
        },
        "id": "OH7borM9-k2p",
        "outputId": "8711f10a-80af-4a3f-ee8c-db682f8dfdb1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14436 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5b61d4768d34ed48835e450de6d559e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "email_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = email_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import Dataset\n",
        "import json\n",
        "# Load your custom dataset\n",
        "json_path = '/content/AESLC/enron_subject_line/test/testdataset.json'\n",
        "\n",
        "with open(json_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "testdataset = Dataset.from_list(data[:100])\n",
        "testdataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ehfiWnH-oSI"
      },
      "source": [
        "#Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRMe_Zxs-l06"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rouge\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bxh2o_v8-1OD",
        "outputId": "8a930b08-5c2a-4eb6-9f00-1f0fadc9bc7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from evaluate import load\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "# Create a text generation pipeline\n",
        "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Generate predictions on the test dataset\n",
        "# Access the input column of the dataset using the column name\n",
        "predictions = generator(\n",
        "    testdataset[:2000][\"input\"],\n",
        "    max_new_tokens=8,\n",
        "    num_beams=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub3-PiJhfqse"
      },
      "outputs": [],
      "source": [
        "# Extract the generated text from the pipeline output\n",
        "predictions = [pred[0]['generated_text'] for pred in predictions]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROUGE metrics\n",
        "results = rouge.compute(predictions=predictions, references=testdataset[:2000][\"output\"])\n",
        "print(results)"
      ],
      "metadata": {
        "id": "INPkWQAPLzxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmqCec7YfrI2",
        "outputId": "91dfc5af-6f73-4723-800c-8dd7360f1b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.052309203096362944, 'rouge2': 0.019196742044162142, 'rougeL': 0.04750068460138787, 'rougeLsum': 0.04933446927139551}\n"
          ]
        }
      ],
      "source": [
        "##Observations - AUG 8th-9th:\n",
        "##--------------------------------------------------\n",
        "# <4min   max_steps = 50    testdataset[:100][\"input\"],max_new_tokens=20,  ----  {'rouge1': 0.04703338794882978, 'rouge2': 0.018094740811992135, 'rougeL': 0.04291153191710728, 'rougeLsum': 0.044421997423522985}\n",
        "# 4mins   max_steps = 50    testdataset[:300][\"input\"],max_new_tokens=20,  ----  {'rouge1': 0.04763686796739476, 'rouge2': 0.018590431359526753, 'rougeL': 0.04346346295047031, 'rougeLsum': 0.04518140308108524}\n",
        "# 5mins   max_steps = 50    testdataset[:400][\"input\"],max_new_tokens=20,  ----  {'rouge1': 0.04841968575892602, 'rouge2': 0.018794301094831288, 'rougeL': 0.04393371452433795, 'rougeLsum': 0.04536474990806128}\n",
        "# 12mins   max_steps = 50    testdataset[:800][\"input\"],max_new_tokens=20,  ---- {'rouge1': 0.04890688956561573, 'rouge2': 0.018216384865342265, 'rougeL': 0.0442624056719164, 'rougeLsum': 0.04646280874733754}\n",
        "# 10mins   max_steps = 40, seed=5000    testdataset[:800][\"input\"],max_new_tokens=20,  ----  {'rouge1': 0.05063502156641153, 'rouge2': 0.018875734537863367, 'rougeL': 0.04545903906916432, 'rougeLsum': 0.04815762881014813}\n",
        "# 8mins   max_steps = 40, seed=5000    testdataset[:800][\"input\"],max_new_tokens=15,  ----  {'rouge1': 0.052136084070747046, 'rouge2': 0.019510195273507815, 'rougeL': 0.04683298782296529, 'rougeLsum': 0.04945309030336453}\n",
        "# 14mins   max_steps = 40, seed=5000   testdataset[:1200][\"input\"],max_new_tokens=20,  ----  {'rouge1': 0.05058661057227369, 'rouge2': 0.019035910262427347, 'rougeL': 0.04597646530256609, 'rougeLsum': 0.04820730672046365}\n",
        "# 7mins   max_steps = 40, seed=5000    testdataset[:1200][\"input\"],max_new_tokens=15,  ----  {'rouge1': 0.05206475367626734, 'rouge2': 0.019660981842852962, 'rougeL': 0.047334418031740016, 'rougeLsum': 0.04955124524476833}\n",
        "# 7mins   max_steps = 40, seed=5000    testdataset[:1200][\"input\"],max_new_tokens=15,  ----  {'rouge1': 0.05378414875142114,  'rouge2': 0.020305912892399518, 'rougeL': 0.04887783094017359,  'rougeLsum': 0.051091497309520784}\n",
        "# 14mins   max_steps = 40, seed=5000   testdataset[:2000][\"input\"],max_new_tokens=10,  ----  {'rouge1': 0.052309203096362944, 'rouge2': 0.019196742044162142, 'rougeL': 0.04750068460138787, 'rougeLsum': 0.04933446927139551}\n",
        "\n",
        "# max_steps = 50    testdataset[:100][\"input\"],max_new_tokens=10,  ----  {'rouge1': 0.05097113185293092, 'rouge2': 0.01915434455190274, 'rougeL': 0.046507767697487296, 'rougeLsum': 0.0481098915021589}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCXYaeqR_EU7"
      },
      "outputs": [],
      "source": [
        "# Save to q4_k_m GGUF\n",
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncD0ai19_FEE"
      },
      "outputs": [],
      "source": [
        "# prompt: copy the unsloth.Q4_K_M.gguf to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/model/unsloth.Q4_K_M.gguf /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio App"
      ],
      "metadata": {
        "id": "l3WPqdc8G9nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "xrJrAAjmG_vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "GCJZk_liHD6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def generate_subject(model_name,email_body):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "  inputs = [\"provide email subject: \" + email_body]\n",
        "  inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "  output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=1, max_length=10)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n",
        "  return predicted_title"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTN-ABc7HJfC",
        "outputId": "faed7dbd-9f4e-4e4a-be6f-821a30a0fc58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=generate_subject,\n",
        "    inputs=[\n",
        "        #CHECK - More options to be added to Gradio\n",
        "        gr.Dropdown(choices=[\"EmailSubGen_Gemma2b_lora_model\", \"anukvma/t5-base-medium-email-subject-generation-v2\", \"anukvma/bart-base-medium-email-subject-generation-v5\"], label=\"Select Model\"),\n",
        "        gr.Textbox(lines=5, label=\"Email Body\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Email Subject\")\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "j6GP5SyjHNde",
        "outputId": "a9762893-1d8f-4463-8290-6f2f72238fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://46ee291de79d4d049b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://46ee291de79d4d049b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ee519403c824836be36d233e17eab6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ee688a29d294cf99bfa17b54142dec3",
              "IPY_MODEL_77d6a1fb9dcf4bc48a3b801c7d916ddf",
              "IPY_MODEL_fe7b4553187540a89fa384059919bafc"
            ],
            "layout": "IPY_MODEL_7b2ec3f21f054834b84d34c8b5bd4e03"
          }
        },
        "5ee688a29d294cf99bfa17b54142dec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9383f2dc2957476d8a72a4b001f51b71",
            "placeholder": "",
            "style": "IPY_MODEL_8a701b7cf899416b99c1d85495bac1ba",
            "value": "Map:100%"
          }
        },
        "77d6a1fb9dcf4bc48a3b801c7d916ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51bac98b3825477ca835b9a1767e07a7",
            "max": 14436,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e629b2d2c9da485a8575b9ff534ae069",
            "value": 14436
          }
        },
        "fe7b4553187540a89fa384059919bafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_527743b06ac842c18f02f70083640735",
            "placeholder": "",
            "style": "IPY_MODEL_19cd3422ce9044a0891ba94b16de6932",
            "value": "14436/14436[00:00&lt;00:00,17426.17examples/s]"
          }
        },
        "7b2ec3f21f054834b84d34c8b5bd4e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9383f2dc2957476d8a72a4b001f51b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a701b7cf899416b99c1d85495bac1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51bac98b3825477ca835b9a1767e07a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e629b2d2c9da485a8575b9ff534ae069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "527743b06ac842c18f02f70083640735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19cd3422ce9044a0891ba94b16de6932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f66021b131f4f44b5cf72275285f021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e78248c57ab74ad4885e9d3717afb463",
              "IPY_MODEL_b8a43fe517a7491081c80946ab101787",
              "IPY_MODEL_6971887d4bd74d228f7af9ae02959845"
            ],
            "layout": "IPY_MODEL_f74040096be342bf8409732a3b4a4325"
          }
        },
        "e78248c57ab74ad4885e9d3717afb463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db89bf602f6144b49a6451f18b5e6ce3",
            "placeholder": "",
            "style": "IPY_MODEL_51fd859e1bc44af2bf37a30702b6c37e",
            "value": "Map(num_proc=2):100%"
          }
        },
        "b8a43fe517a7491081c80946ab101787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fddd59b738742998abc8627be17683e",
            "max": 14436,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13cd6061f14d44b7a3981dd615a98d72",
            "value": 14436
          }
        },
        "6971887d4bd74d228f7af9ae02959845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_866f6e10849d4b8ab9634cc5cba28315",
            "placeholder": "",
            "style": "IPY_MODEL_5d4a573ee3714f46a7e4260a2ad78b42",
            "value": "14436/14436[00:42&lt;00:00,719.38examples/s]"
          }
        },
        "f74040096be342bf8409732a3b4a4325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db89bf602f6144b49a6451f18b5e6ce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51fd859e1bc44af2bf37a30702b6c37e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fddd59b738742998abc8627be17683e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13cd6061f14d44b7a3981dd615a98d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "866f6e10849d4b8ab9634cc5cba28315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d4a573ee3714f46a7e4260a2ad78b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5b61d4768d34ed48835e450de6d559e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b60f8b553948480e812f22630de26751",
              "IPY_MODEL_1236b75266ff49ffa0d86394f1abb999",
              "IPY_MODEL_3acb04584353414b9eeeac2c0d0b4e42"
            ],
            "layout": "IPY_MODEL_4d6ac5e2a1304bbb9947301045531b06"
          }
        },
        "b60f8b553948480e812f22630de26751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e777897fc5840a0aeae26eae70d868a",
            "placeholder": "",
            "style": "IPY_MODEL_ed580b749f7140dbbb1f2dc2d0e970c6",
            "value": "Map:100%"
          }
        },
        "1236b75266ff49ffa0d86394f1abb999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07469b6eb8194f68824ef2d4aa2027cd",
            "max": 14436,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da1525c981504bdb8da5338fa716591b",
            "value": 14436
          }
        },
        "3acb04584353414b9eeeac2c0d0b4e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bfdf8fa01c94d129ded70f8d5f847a2",
            "placeholder": "",
            "style": "IPY_MODEL_2e0be2e3baaa424ea13913b41902b2fa",
            "value": "14436/14436[00:00&lt;00:00,75584.62examples/s]"
          }
        },
        "4d6ac5e2a1304bbb9947301045531b06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e777897fc5840a0aeae26eae70d868a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed580b749f7140dbbb1f2dc2d0e970c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07469b6eb8194f68824ef2d4aa2027cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da1525c981504bdb8da5338fa716591b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bfdf8fa01c94d129ded70f8d5f847a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e0be2e3baaa424ea13913b41902b2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
